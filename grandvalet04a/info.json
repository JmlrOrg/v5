{
    "abstract": "Most machine learning researchers perform quantitative experiments \nto estimate generalization error and compare the performance of \ndifferent algorithms (in particular, their proposed algorithm). In \norder to be able to draw statistically convincing conclusions, \nit is important  to estimate the uncertainty of such estimates.\nThis paper studies the very commonly used K-fold cross-validation \nestimator of generalization performance. The main theorem shows \nthat there exists no universal (valid under all distributions) \nunbiased estimator of the variance of K-fold cross-validation. \nThe analysis that accompanies this result is based on the \neigen-decomposition of the covariance matrix of errors, which \nhas only three different eigenvalues corresponding to three \ndegrees of freedom of the matrix and three components of the \ntotal variance. This analysis helps to better understand the \nnature of the problem and how it can make naive estimators \n(that don't take into account the error correlations due to \nthe overlap between training and test sets) grossly underestimate \nvariance. This is confirmed by numerical experiments in which \nthe three components of the variance are compared when the \ndifficulty of the learning problem and the number of folds are varied.",
    "authors": [
        "Yoshua Bengio",
        "Yves Grandvalet"
    ],
    "id": "grandvalet04a",
    "issue": 39,
    "pages": [
        1089,
        1105
    ],
    "title": "No Unbiased Estimator of the Variance of K-Fold Cross-Validation",
    "volume": "5",
    "year": "2004"
}